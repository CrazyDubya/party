# Unify AI

## Overview
Unify AI is a platform designed for building, testing, and optimizing LLM pipelines. It provides a unified API for accessing models across different providers and offers usage-based pricing for inference, along with options for dedicated instances.

## Models Offered
| Model Name | Context Window (Tokens) | Cost per 1M Input Tokens | Cost per 1M Output Tokens | Cache Support | Output Size Limit | Notes |
|---|---|---|---|---|---|---|
| (Various LLMs from different providers) | Varies by model | Varies by model | Varies by model | Not explicitly mentioned | Not explicitly mentioned | Unify AI provides access to models from various providers. Specific model details depend on the integrated LLMs. |

## Key Features
*   **API Endpoints:** Unified API for accessing models across different providers.
*   **Pricing Model:** Usage-based (per 1 million tokens). Also offers dedicated instances.
*   **Caching:** Not explicitly mentioned in the search results.
*   **Rate Limits:** Not explicitly mentioned in the search results.
*   **Supported Modalities:** Primarily text-based LLMs, but depends on the integrated models.
*   **Developer Tools/SDKs:** Platform for building, testing, and optimizing LLM pipelines.
*   **Unique Selling Propositions:** Focus on LLM pipeline optimization, unified API for multi-provider access, dedicated instance options.

## Pros & Cons
*   **Pros:**
    *   Simplifies LLM pipeline development and optimization.
    *   Unified API for accessing models from various providers.
    *   Flexible pricing with usage-based and dedicated instance options.
*   **Cons:**
    *   Specific model details (context window, token costs) are not transparently listed on their public website.
    *   Less focus on being a direct inference provider for a wide range of models, more on orchestration.

## Links
*   [Official Website](https://unify.ai/)